{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out filler pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file filler.txt indicates which pages are filler (e.g. cover, foreword, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFillerList(filler_file, feat_dir):\n",
    "    d = {} # list of pages to remove\n",
    "    with open(filler_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) > 1:\n",
    "                relpath = parts[0] # e.g. Bach/00748\n",
    "                scoreID = os.path.basename(relpath) # e.g. 00748\n",
    "                removeField = parts[1].strip('\"') # e.g. \"0,1,-2,-1\" or \"r\" or \"rl\"\n",
    "                numPages = getNumPages(relpath, feat_dir)\n",
    "                if removeField == 'r' or removeField == 'rl': # remove all pages\n",
    "                    for pkl_file in glob.glob('{}/{}/*.pkl'.format(feat_dir, parts[0])):\n",
    "                        pageID = os.path.splitext(os.path.basename(pkl_file))[0] # e.g. 00822-3\n",
    "                        d[pageID] = 1\n",
    "                else:\n",
    "                    for pageNumStr in removeField.split(','):\n",
    "                        pageNum = int(pageNumStr)\n",
    "                        if pageNum < 0:\n",
    "                            pageID = '{}-{}'.format(scoreID, numPages + pageNum)\n",
    "                            d[pageID] = 1\n",
    "                        else:\n",
    "                            pageID = '{}-{}'.format(scoreID, pageNum)\n",
    "                            d[pageID] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumPages(relpath, indir):\n",
    "    numPages = len(glob.glob('{}/{}/*.pkl'.format(indir, relpath)))\n",
    "    return numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNonFillerFeatures(filler_file, feat_dir):\n",
    "    '''\n",
    "    Collect bootleg score features from all pages that are (a) not filler and (b) have a valid \n",
    "    bootleg score matrix.\n",
    "    '''\n",
    "    \n",
    "    filler = getFillerList(filler_file, feat_dir)\n",
    "    feats = {}\n",
    "    \n",
    "    for pieceDir in glob.glob('{}/*/*/'.format(feat_dir)): # e.g. score_feat/Bach/00748/\n",
    "        \n",
    "        pieceID = pieceDir.split('/')[-2]\n",
    "        composer = pieceDir.split('/')[-3]\n",
    "        accum = [] # collect features from all valid pages in this score\n",
    "        \n",
    "        for pkl_file in glob.glob('{}/*.pkl'.format(pieceDir)):\n",
    "            \n",
    "            pageID = os.path.splitext(os.path.basename(pkl_file))[0] # e.g. 00748-2\n",
    "            if pageID in filler: # filler page, skip\n",
    "                continue\n",
    "            with open(pkl_file, 'rb') as f:\n",
    "                bscore = pickle.load(f)['bscore']\n",
    "            if bscore is not None: # if None, no features were computed\n",
    "                accum.append(bscore == 1) # convert from float to bool to compress memory\n",
    "        \n",
    "        if len(accum) > 0:\n",
    "            feats[pieceDir] = accum\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filler_file = 'cfg_files/filler.txt'\n",
    "score_feat_dir = 'score_feat'\n",
    "feats = getNonFillerFeatures(filler_file, score_feat_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureStats(feats):\n",
    "    \n",
    "    # count number of features per page\n",
    "    featsPerPage = []\n",
    "    for pieceDir in feats:\n",
    "        for elem in feats[pieceDir]:\n",
    "            featsPerPage.append(elem.shape[1])\n",
    "    featsPerPage = np.array(featsPerPage)\n",
    "    printStats(featsPerPage, \"Number of Features Per Page\")\n",
    "    \n",
    "    # plot histogram\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.hist(featsPerPage, bins=100)\n",
    "    plt.xlabel('Number of Events In Single Page')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # count total number of pages by composer\n",
    "    pages = {}\n",
    "    for pieceDir in feats: # e.g. score_feat/Bach/00748/\n",
    "        composer = pieceDir.split('/')[-3]\n",
    "        if composer not in pages:\n",
    "            pages[composer] = 0\n",
    "        pages[composer] += len(feats[pieceDir])\n",
    "    pageCnts = [pages[composer] for composer in pages]\n",
    "    composers = [composer[0:5] for composer in pages]\n",
    "    printStats(pageCnts, \"Total Number of Pages by Composer\")\n",
    "    \n",
    "    # plot histogram\n",
    "    x_pos = np.arange(len(pageCnts))\n",
    "    plt.bar(x_pos, pageCnts)\n",
    "    plt.xticks(x_pos, composers)\n",
    "    plt.ylabel('Total # Pages')\n",
    "    plt.show()\n",
    "    \n",
    "    # count total number of note events by composer\n",
    "    noteEvents = {}\n",
    "    for pieceDir in feats: # e.g. score_feat/Bach/00748/\n",
    "        composer = pieceDir.split('/')[-3]\n",
    "        if composer not in noteEvents:\n",
    "            noteEvents[composer] = 0\n",
    "        for elem in feats[pieceDir]:\n",
    "            noteEvents[composer] += elem.shape[1]\n",
    "    noteEventCnts = [noteEvents[composer] for composer in noteEvents]\n",
    "    printStats(noteEventCnts, \"Total Number of Note Events by Composer\")\n",
    "    \n",
    "    # plot histogram\n",
    "    x_pos = np.arange(len(composers))\n",
    "    plt.bar(x_pos, noteEventCnts)\n",
    "    plt.xticks(x_pos, composers)\n",
    "    plt.ylabel('Total # Note Events')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printStats(arr, title = None):\n",
    "    if title:\n",
    "        print(title)\n",
    "    print('Mean: {}'.format(np.mean(arr)))\n",
    "    print('Std: {}'.format(np.std(arr)))\n",
    "    print('Min: {}'.format(np.min(arr)))\n",
    "    print('Max: {}'.format(np.max(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFeatureStats(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Train, Validation, & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainValidTest(d, train=.6, validation=.2, test=.2, savefile = None):\n",
    "    \n",
    "    # shuffle\n",
    "    assert(train + validation + test == 1.0)\n",
    "    np.random.seed(0)\n",
    "    pieceDirs = list(d.keys())\n",
    "    np.random.shuffle(pieceDirs)\n",
    "    \n",
    "    # split\n",
    "    breakpt1 = int(len(pieceDirs) * train)\n",
    "    breakpt2 = breakpt1 + int(len(pieceDirs) * validation)\n",
    "    pieceDirs_train = pieceDirs[0:breakpt1]\n",
    "    pieceDirs_valid = pieceDirs[breakpt1:breakpt2]\n",
    "    pieceDirs_test = pieceDirs[breakpt2:]\n",
    "    \n",
    "    # save\n",
    "    d_train = getDataSubset(d, pieceDirs_train)\n",
    "    d_valid = getDataSubset(d, pieceDirs_valid)\n",
    "    d_test = getDataSubset(d, pieceDirs_test)\n",
    "    if savefile:\n",
    "        saveToPickle([d, pieceDirs_train, pieceDirs_valid, pieceDirs_test], savefile)\n",
    "    \n",
    "    return d_train, d_valid, d_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataSubset(dAll, toKeep):\n",
    "    dSubset = {}\n",
    "    for pieceDir in toKeep:\n",
    "        dSubset[pieceDir] = dAll[pieceDir]\n",
    "    return dSubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveToPickle(d, outfile):\n",
    "    with open(outfile, 'wb') as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPickle(infile):\n",
    "    with open(infile, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pages_file = '{}/data.pages.pkl'.format(score_feat_dir)\n",
    "d_train, d_valid, d_test = splitTrainValidTest(feats, train=.6, validation=.2, test=.2, savefile=save_pages_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getComposer2IndexMapping(feat_dir):\n",
    "    composers = []\n",
    "    for composerDir in sorted(glob.glob('{}/*/'.format(feat_dir))):\n",
    "        composer = composerDir.split('/')[-2]\n",
    "        composers.append(composer)\n",
    "    c_to_i = {c:i for i, c in enumerate(composers)}\n",
    "    \n",
    "    return c_to_i, composers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChunkedData(d, chunkSize, c_to_i):\n",
    "    frags = []\n",
    "    labels = []\n",
    "    pieceDir2idxRange = {}\n",
    "    for pieceDir in d:\n",
    "        merged = np.hstack(d[pieceDir])\n",
    "        composerIdx = c_to_i[pieceDir.split('/')[-3]]\n",
    "        startChunkIdx = len(frags)\n",
    "        for startIdx in range(0, merged.shape[1], chunkSize // 2):\n",
    "            endIdx = startIdx + chunkSize\n",
    "            if endIdx <= merged.shape[1]:\n",
    "                frags.append(merged[:,startIdx:endIdx])\n",
    "                labels.append(composerIdx)\n",
    "        endChunkIdx = len(frags)\n",
    "        pieceDir2idxRange[pieceDir] = (startChunkIdx, endChunkIdx)\n",
    "    frags = np.array(frags)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return frags, labels, pieceDir2idxRange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer2idx, idx2composer = getComposer2IndexMapping(score_feat_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkSize = 64\n",
    "X_train, y_train, map_train = getChunkedData(d_train, chunkSize, composer2idx)\n",
    "X_valid, y_valid, map_valid = getChunkedData(d_valid, chunkSize, composer2idx)\n",
    "X_test, y_test, map_test = getChunkedData(d_test, chunkSize, composer2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chunks_file = '{}/data.chunks.pkl'.format(score_feat_dir)\n",
    "saveToPickle([X_train, y_train, map_train, X_valid, y_valid, map_valid, X_test, y_test, map_test], save_chunks_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PianoStyleEmbedding",
   "language": "python",
   "name": "pianostyleembedding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
